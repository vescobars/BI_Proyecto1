{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-profiling\n",
      "  Downloading pandas_profiling-3.2.0-py2.py3-none-any.whl (262 kB)\n",
      "     |████████████████████████████████| 262 kB 29.2 MB/s            \n",
      "\u001b[?25hCollecting seaborn>=0.10.1\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "     |████████████████████████████████| 292 kB 129.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: joblib~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (1.1.0)\n",
      "Collecting missingno>=0.4.2\n",
      "  Downloading missingno-0.5.1-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (4.48.2)\n",
      "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (2.24.0)\n",
      "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (1.1.5)\n",
      "Collecting tangled-up-in-unicode==0.2.0\n",
      "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
      "     |████████████████████████████████| 4.7 MB 111.6 MB/s            \n",
      "\u001b[?25hCollecting htmlmin>=0.1.12\n",
      "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pydantic>=1.8.1\n",
      "  Downloading pydantic-1.9.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
      "     |████████████████████████████████| 11.2 MB 39.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (1.4.1)\n",
      "Requirement already satisfied: PyYAML>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (5.3.1)\n",
      "Collecting visions[type_image_path]==0.7.4\n",
      "  Downloading visions-0.7.4-py3-none-any.whl (102 kB)\n",
      "     |████████████████████████████████| 102 kB 26.7 MB/s            \n",
      "\u001b[?25hCollecting pandas-profiling\n",
      "  Downloading pandas_profiling-3.1.0-py2.py3-none-any.whl (261 kB)\n",
      "     |████████████████████████████████| 261 kB 117.8 MB/s            \n",
      "\u001b[?25hCollecting joblib~=1.0.1\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "     |████████████████████████████████| 303 kB 98.0 MB/s            \n",
      "\u001b[?25hCollecting multimethod>=1.4\n",
      "  Downloading multimethod-1.5-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (1.19.5)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (2.11.2)\n",
      "Collecting tangled-up-in-unicode==0.1.0\n",
      "  Downloading tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1 MB)\n",
      "     |████████████████████████████████| 3.1 MB 41.2 MB/s            \n",
      "\u001b[?25hCollecting markupsafe~=2.0.1\n",
      "  Downloading MarkupSafe-2.0.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (3.3.4)\n",
      "Collecting phik>=0.11.1\n",
      "  Downloading phik-0.12.0-cp36-cp36m-manylinux2010_x86_64.whl (675 kB)\n",
      "     |████████████████████████████████| 675 kB 103.5 MB/s            \n",
      "\u001b[?25hCollecting networkx>=2.4\n",
      "  Downloading networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
      "     |████████████████████████████████| 1.6 MB 96.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.6/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (19.3.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (8.4.0)\n",
      "Collecting imagehash\n",
      "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "     |████████████████████████████████| 296 kB 106.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2021.3)\n",
      "Collecting scipy>=1.4.1\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "     |████████████████████████████████| 25.9 MB 38.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.6/dist-packages (from pydantic>=1.8.1->pandas-profiling) (0.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.6/dist-packages (from pydantic>=1.8.1->pandas-profiling) (4.1.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.24.0->pandas-profiling) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.24.0->pandas-profiling) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.24.0->pandas-profiling) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.24.0->pandas-profiling) (2.10)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.4->visions[type_image_path]==0.7.4->pandas-profiling) (4.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.0->pandas-profiling) (1.15.0)\n",
      "Collecting PyWavelets\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "     |████████████████████████████████| 4.4 MB 40.0 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: htmlmin\n",
      "  Building wheel for htmlmin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27084 sha256=a901ebdc4914036c212d2a8498f2ce025141c5e407b4a34ca6eb514ed0003229\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/fe/0b/4450b38bceb9ae43dd7d0f16e353566f30f5f4d59a58eca2ed\n",
      "Successfully built htmlmin\n",
      "Installing collected packages: tangled-up-in-unicode, scipy, PyWavelets, networkx, multimethod, visions, seaborn, markupsafe, joblib, imagehash, pydantic, phik, missingno, htmlmin, pandas-profiling\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 1.1.1\n",
      "    Uninstalling MarkupSafe-1.1.1:\n",
      "      Successfully uninstalled MarkupSafe-1.1.1\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.2.0+nv requires scipy==1.4.1; python_version >= \"3\", but you have scipy 1.5.4 which is incompatible.\u001b[0m\n",
      "Successfully installed PyWavelets-1.1.1 htmlmin-0.1.12 imagehash-4.3.1 joblib-1.0.1 markupsafe-2.0.1 missingno-0.5.1 multimethod-1.5 networkx-2.5.1 pandas-profiling-3.1.0 phik-0.12.0 pydantic-1.9.2 scipy-1.5.4 seaborn-0.11.2 tangled-up-in-unicode-0.1.0 visions-0.7.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.4-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "     |████████████████████████████████| 106 kB 57.0 MB/s            \n",
      "\u001b[?25hCollecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "     |████████████████████████████████| 287 kB 90.3 MB/s            \n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.24\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting inflect\n",
      "  Downloading inflect-5.3.0-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: inflect\n",
      "Successfully installed inflect-5.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas-profiling\n",
    "%pip install contractions\n",
    "%pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for text processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pandas_profiling import ProfileReport\n",
    "import re, string, unicodedata, contractions, inflect\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_precision_recall_curve\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # librería Natural Language Toolkit, usada para trabajar con textos \n",
    "import nltk\n",
    "# Punkt permite separar un texto en frases.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('SuicidiosProyecto.csv', sep=',', encoding = 'utf-8')\n",
    "data_t = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def process_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "    # Remove tags\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n",
    "    # only removing the hash # sign from the word\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # Contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Tokenize text\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    texts_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    texts_clean = []\n",
    "    for word in texts_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            texts_clean.append(stem_word)\n",
    "\n",
    "    return texts_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t['processed_text'] = data_t['text'].apply(lambda x: process_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173271</td>\n",
       "      <td>i want to destroy myselffor once everything wa...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>[want, destroy, myselffor, everyth, start, fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>336321</td>\n",
       "      <td>I kinda got behind schedule with learning for ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[kind, got, behind, schedul, learn, next, week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256637</td>\n",
       "      <td>I'm just not sure anymoreFirst and foremost: I...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>[sure, anymorefirst, foremost, brazil, judg, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303772</td>\n",
       "      <td>please give me a reason to liveThats too much ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>[pleas, give, reason, livethat, much, reason, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>293747</td>\n",
       "      <td>27f struggling to find meaning moving forwardI...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>[f, struggl, find, mean, move, forwardi, admit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text        class  \\\n",
       "0      173271  i want to destroy myselffor once everything wa...      suicide   \n",
       "1      336321  I kinda got behind schedule with learning for ...  non-suicide   \n",
       "2      256637  I'm just not sure anymoreFirst and foremost: I...      suicide   \n",
       "3      303772  please give me a reason to liveThats too much ...      suicide   \n",
       "4      293747  27f struggling to find meaning moving forwardI...      suicide   \n",
       "\n",
       "                                      processed_text  \n",
       "0  [want, destroy, myselffor, everyth, start, fee...  \n",
       "1  [kind, got, behind, schedul, learn, next, week...  \n",
       "2  [sure, anymorefirst, foremost, brazil, judg, s...  \n",
       "3  [pleas, give, reason, livethat, much, reason, ...  \n",
       "4  [f, struggl, find, mean, move, forwardi, admit...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173271</td>\n",
       "      <td>i want to destroy myselffor once everything wa...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>want destroy myselffor everyth start feel okay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>336321</td>\n",
       "      <td>I kinda got behind schedule with learning for ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>kind got behind schedul learn next week testwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256637</td>\n",
       "      <td>I'm just not sure anymoreFirst and foremost: I...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>sure anymorefirst foremost brazil judg second ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303772</td>\n",
       "      <td>please give me a reason to liveThats too much ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>pleas give reason livethat much reason live li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>293747</td>\n",
       "      <td>27f struggling to find meaning moving forwardI...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>f struggl find mean move forwardi admit bit lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195695</th>\n",
       "      <td>248038</td>\n",
       "      <td>Drop some cool new cereal ideas Like what woul...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>drop cool new cereal idea like would ideal cereal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195696</th>\n",
       "      <td>216516</td>\n",
       "      <td>Unpopular opinion but cats deserve love and re...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>unpopular opinion cat deserv love respect much...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195697</th>\n",
       "      <td>199341</td>\n",
       "      <td>Hey guys :) How yall doin?</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>hey guy doin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195698</th>\n",
       "      <td>145373</td>\n",
       "      <td>uhm I covered my dog in a blanket because the ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>uhm cover dog blanket light wake woke ran wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195699</th>\n",
       "      <td>305170</td>\n",
       "      <td>____god. how do i do it. how do i end my life....</td>\n",
       "      <td>suicide</td>\n",
       "      <td>____god end life tire want anyth need someon c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195700 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  \\\n",
       "0           173271  i want to destroy myselffor once everything wa...   \n",
       "1           336321  I kinda got behind schedule with learning for ...   \n",
       "2           256637  I'm just not sure anymoreFirst and foremost: I...   \n",
       "3           303772  please give me a reason to liveThats too much ...   \n",
       "4           293747  27f struggling to find meaning moving forwardI...   \n",
       "...            ...                                                ...   \n",
       "195695      248038  Drop some cool new cereal ideas Like what woul...   \n",
       "195696      216516  Unpopular opinion but cats deserve love and re...   \n",
       "195697      199341                         Hey guys :) How yall doin?   \n",
       "195698      145373  uhm I covered my dog in a blanket because the ...   \n",
       "195699      305170  ____god. how do i do it. how do i end my life....   \n",
       "\n",
       "              class                                     processed_text  \n",
       "0           suicide  want destroy myselffor everyth start feel okay...  \n",
       "1       non-suicide  kind got behind schedul learn next week testwe...  \n",
       "2           suicide  sure anymorefirst foremost brazil judg second ...  \n",
       "3           suicide  pleas give reason livethat much reason live li...  \n",
       "4           suicide  f struggl find mean move forwardi admit bit lo...  \n",
       "...             ...                                                ...  \n",
       "195695  non-suicide  drop cool new cereal idea like would ideal cereal  \n",
       "195696  non-suicide  unpopular opinion cat deserv love respect much...  \n",
       "195697  non-suicide                                       hey guy doin  \n",
       "195698  non-suicide     uhm cover dog blanket light wake woke ran wall  \n",
       "195699      suicide  ____god end life tire want anyth need someon c...  \n",
       "\n",
       "[195700 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_t['processed_text'] = data_t['processed_text'].apply(lambda x: ' '.join(map(str, x)))\n",
    "data_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_t['processed_text'], data_t['class']\n",
    "y = (y == 'suicide').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21358     1\n",
       "65465     1\n",
       "164476    0\n",
       "165082    1\n",
       "75228     1\n",
       "         ..\n",
       "119879    0\n",
       "103694    0\n",
       "131932    0\n",
       "146867    1\n",
       "121958    0\n",
       "Name: class, Length: 156560, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,3))\n",
    "\n",
    "X_tfidf_train= tfidf_vectorizer.fit_transform(X_train)\n",
    "X_tfidf_test=tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC (Support Vector Machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(C=100, penalty='l1', max_iter=1000, dual=False)\n",
    "lsvc.fit(X_tfidf_train, y_train)\n",
    "\n",
    "# Select the best features that has high weight\n",
    "fs = SelectFromModel(lsvc, prefit=True)\n",
    "X_selection = fs.transform(X_tfidf_train)\n",
    "X_test_selection = fs.transform(X_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc_tfidf = LinearSVC(C=1000, penalty='l1', max_iter=1000, dual=False)\n",
    "\n",
    "lsvc_tfidf.fit(X_selection, y_train)\n",
    "y_predict_tfidf = lsvc_tfidf.predict(X_test_selection)\n",
    "\n",
    "linear_svm_tfidf_results = metrics.precision_recall_fscore_support(y_test, y_predict_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20770  1299]\n",
      " [ 1609 15462]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     22069\n",
      "           1       0.92      0.91      0.91     17071\n",
      "\n",
      "    accuracy                           0.93     39140\n",
      "   macro avg       0.93      0.92      0.92     39140\n",
      "weighted avg       0.93      0.93      0.93     39140\n",
      "\n",
      "Accuracy:  0.9257026060296372\n",
      "F1 score:  [0.93457523 0.91404587]\n"
     ]
    }
   ],
   "source": [
    "# Show confusion matrix\n",
    "cm = confusion_matrix(y_test, y_predict_tfidf)\n",
    "print(cm)\n",
    "\n",
    "# Show precision and recall\n",
    "print(classification_report(y_test, y_predict_tfidf))\n",
    "\n",
    "# Show accuracy\n",
    "print('Accuracy: ', tfidf_acc)\n",
    "\n",
    "# Show f1 score\n",
    "print('F1 score: ', linear_svm_tfidf_results[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predict function\n",
    "def predict(text):\n",
    "    text = process_text(text)\n",
    "    text = ' '.join(map(str, text))\n",
    "    text = [text]\n",
    "    text = tfidf_vectorizer.transform(text)\n",
    "    text = fs.transform(text)\n",
    "    return lsvc_tfidf.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, vectorizer and feature selection\n",
    "import pickle\n",
    "pickle.dump(lsvc_tfidf, open('model_svm.pkl','wb'))\n",
    "pickle.dump(tfidf_vectorizer, open('tfidf_vectorizer.pkl','wb'))\n",
    "pickle.dump(fs, open('feature_selection.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model, vectorizer and feature selection\n",
    "import pickle\n",
    "model_svm = pickle.load(open('model_svm.pkl','rb'))\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl','rb'))\n",
    "fs = pickle.load(open('feature_selection.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.9414406322974672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "lr.fit(X_tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20964  1105]\n",
      " [ 1503 15568]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     22069\n",
      "           1       0.93      0.91      0.92     17071\n",
      "\n",
      "    accuracy                           0.93     39140\n",
      "   macro avg       0.93      0.93      0.93     39140\n",
      "weighted avg       0.93      0.93      0.93     39140\n",
      "\n",
      "F1 score:  0.9414406322974672\n"
     ]
    }
   ],
   "source": [
    "y_predict_lr = lr.predict(X_tfidf_test)\n",
    "lr_results = metrics.precision_recall_fscore_support(y_test, y_predict_lr)\n",
    "\n",
    "# Show confusion matrix\n",
    "cm = confusion_matrix(y_test, y_predict_lr)\n",
    "print(cm)\n",
    "\n",
    "# Show precision and recall\n",
    "print(classification_report(y_test, y_predict_lr))\n",
    "\n",
    "# Show f1 score\n",
    "print('F1 score: ', lr_results[2][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lr(text):\n",
    "    text = process_text(text)\n",
    "    text = ' '.join(map(str, text))\n",
    "    text = [text]\n",
    "    text = tfidf_vectorizer.transform(text)\n",
    "    return lr.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(lr, open('model_lr.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_tfidf_train, y_train)\n",
    "\n",
    "y_predict_nb = nb.predict(X_tfidf_test)\n",
    "nb_results = metrics.precision_recall_fscore_support(y_test, y_predict_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19748  2321]\n",
      " [ 1172 15899]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.92     22069\n",
      "           1       0.87      0.93      0.90     17071\n",
      "\n",
      "    accuracy                           0.91     39140\n",
      "   macro avg       0.91      0.91      0.91     39140\n",
      "weighted avg       0.91      0.91      0.91     39140\n",
      "\n",
      "F1 score:  0.9187466561213333\n"
     ]
    }
   ],
   "source": [
    "y_predict_nb = nb.predict(X_tfidf_test)\n",
    "lr_results = metrics.precision_recall_fscore_support(y_test, y_predict_nb)\n",
    "\n",
    "# Show confusion matrix\n",
    "cm = confusion_matrix(y_test, y_predict_nb)\n",
    "print(cm)\n",
    "\n",
    "# Show precision and recall\n",
    "print(classification_report(y_test, y_predict_nb))\n",
    "\n",
    "# Show f1 score\n",
    "print('F1 score: ', lr_results[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nb(text):\n",
    "    text = process_text(text)\n",
    "    text = ' '.join(map(str, text))\n",
    "    text = [text]\n",
    "    text = tfidf_vectorizer.transform(text)\n",
    "    return nb.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(nb, open('model_nb.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=20, random_state=0)\n",
    "rf.fit(X_tfidf_train, y_train)\n",
    "\n",
    "y_predict_rf = rf.predict(X_tfidf_test)\n",
    "rf_results = metrics.precision_recall_fscore_support(y_test, y_predict_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.83547035, 0.89405064]),\n",
       " array([0.93003761, 0.76322418]),\n",
       " array([0.88022129, 0.82347364]),\n",
       " array([22069, 17071]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a0efc7ed7210c8daa9aadd584bbae766f571d407ba1506112b0e572af6a4d67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
